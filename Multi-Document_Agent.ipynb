{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9625ab2-71b6-4fd0-904e-42df80d3215f",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3221a474-5817-4db2-af46-e029042a75a5",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed10a24b-d65c-4b98-a93a-94ccdb8900d0",
   "metadata": {
    "height": 200,
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f3185-3221-4b00-bd38-41d36e4a3307",
   "metadata": {
    "height": 149,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e541bdd-14e1-41b6-81b5-b1bfda078d07",
   "metadata": {
    "height": 30,
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bff58c52",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f2c6a9f",
   "metadata": {
    "height": 30
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(initial_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a124a438-5609-402e-8642-69d1088cb9ad",
   "metadata": {
    "height": 166,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17409d4c-05a9-4bf4-b74f-75135fa3cb6b",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\", \"page_numbers\": [\"19\"]}\n",
      "=== Function Output ===\n",
      "The proposed framework achieves state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the model.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is not explicitly mentioned in the paper. However, the proposed framework achieved state-of-the-art performance on cross-dataset and cross-manipulation evaluations, demonstrating the effectiveness and generalization of the model.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ace340b1-761f-4058-be41-68cf131541e4",
   "metadata": {
    "height": 47,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It enables the language model to adaptively retrieve passages, generate text, and evaluate its own outputs using special tokens known as reflection tokens. By training a single LM to retrieve, generate, and reflect on text passages, Self-RAG significantly improves response generation performance across various tasks compared to state-of-the-art language models and retrieval-augmented models.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in handling longer contexts, demonstrating strong empirical results on various tasks and models. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "=== LLM Response ===\n",
      "Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It enables the language model to adaptively retrieve passages, generate text, and evaluate its own outputs using special tokens known as reflection tokens. By training a single LM to retrieve, generate, and reflect on text passages, Self-RAG significantly improves response generation performance across various tasks compared to state-of-the-art language models and retrieval-augmented models.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in handling longer contexts, demonstrating strong empirical results on various tasks and models. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n",
      "assistant: Self-RAG is a framework that enhances the quality and factuality of a large language model by incorporating retrieval and self-reflection mechanisms. It enables the language model to adaptively retrieve passages, generate text, and evaluate its own outputs using special tokens known as reflection tokens. By training a single LM to retrieve, generate, and reflect on text passages, Self-RAG significantly improves response generation performance across various tasks compared to state-of-the-art language models and retrieval-augmented models.\n",
      "\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings without compromising performance. LongLoRA has been shown to be effective in handling longer contexts, demonstrating strong empirical results on various tasks and models. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a250cf1a-e011-4994-bcca-4e0294f20864",
   "metadata": {
    "height": 98,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_swebench with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "The evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including issues and pull requests from Python repositories. It includes various components such as task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide challenging software engineering tasks for evaluating language models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama. The models are assessed based on their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. Task instances are validated through execution-based validation to ensure usability, with finalized instances saved in a .json file that is open-sourced and available for download.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including issues and pull requests from Python repositories. It includes various components such as task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide challenging software engineering tasks for evaluating language models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama. The models are assessed based on their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. Task instances are validated through execution-based validation to ensure usability, with finalized instances saved in a .json file that is open-sourced and available for download.\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench consists of task instances collected from real GitHub repositories, including issues and pull requests from Python repositories. It includes various components such as task instructions, issue text, retrieved files, documentation, example patch files, and prompts for generating patch files. The dataset aims to provide challenging software engineering tasks for evaluating language models like ChatGPT-3.5, GPT-4, Claude 2, and SWE-Llama. The models are assessed based on their performance in resolving issues using BM25 retrieval and the \"oracle\" retrieval setting. Task instances are validated through execution-based validation to ensure usability, with finalized instances saved in a .json file that is open-sourced and available for download.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8084c8cb-98ed-4835-aaa4-5b0c7254be6d",
   "metadata": {
    "height": 81,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Compare and contrast the LoRA papers (LongLoRA, LoftQ). Analyze the approach in each paper first. \n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n",
      "=== Function Output ===\n",
      "LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, allowing for significant computation savings compared to full fine-tuning. LongLoRA aims to bridge the performance gap between short and long context lengths in LLMs, demonstrating strong empirical results on various tasks and models. Additionally, it retains the original model architectures and is compatible with existing techniques like Flash-Attention2, making it a popular choice for adapting LLMs to different datasets efficiently.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_loftq with args: {\"input\": \"LoftQ\"}\n",
      "=== Function Output ===\n",
      "LoftQ is a quantization framework designed for Large Language Models (LLMs) that integrates quantization and Low-Rank Adaptation (LoRA) fine-tuning. It aims to bridge the performance gap observed when applying quantization and LoRA fine-tuning together on pre-trained models. LoftQ simultaneously quantizes LLMs and finds suitable low-rank initializations for LoRA fine-tuning, enhancing generalization in downstream tasks and showing superior performance compared to existing quantization methods, especially in challenging low-bit scenarios. The framework involves quantizing weight matrices, obtaining low-rank approximations, and efficiently approximating solutions to improve performance by mitigating quantization discrepancies. During LoRA fine-tuning, LoftQ freezes integer weights and optimizes low-rank adapters using algorithms like AdamW, reducing training costs and consistently outperforming other baseline methods in various tasks and datasets. Additionally, LoftQ is a model compression technique that focuses on reducing the size of pre-trained models while maintaining performance, utilizing different quantization methods like Uniform and NF2/NF4 to impact compression ratio, trainable parameters, and execution time of the models.\n",
      "=== LLM Response ===\n",
      "The LongLoRA paper focuses on extending the context length of Large Language Models (LLMs) efficiently while minimizing computational costs. It combines shifted sparse attention (S2-Attn) with LoRA to achieve this extension, resulting in significant computation savings compared to full fine-tuning. LongLoRA aims to bridge the performance gap between short and long context lengths in LLMs, demonstrating strong empirical results on various tasks and models. It retains the original model architectures and is compatible with existing techniques like Flash-Attention2, making it a popular choice for adapting LLMs to different datasets efficiently.\n",
      "\n",
      "On the other hand, the LoftQ paper introduces a quantization framework for LLMs that integrates quantization and Low-Rank Adaptation (LoRA) fine-tuning. LoftQ aims to bridge the performance gap observed when applying quantization and LoRA fine-tuning together on pre-trained models. It simultaneously quantizes LLMs and finds suitable low-rank initializations for LoRA fine-tuning, enhancing generalization in downstream tasks and showing superior performance compared to existing quantization methods, especially in challenging low-bit scenarios. LoftQ involves quantizing weight matrices, obtaining low-rank approximations, and efficiently approximating solutions to improve performance by mitigating quantization discrepancies. During LoRA fine-tuning, LoftQ freezes integer weights and optimizes low-rank adapters using algorithms like AdamW, reducing training costs and consistently outperforming other baseline methods in various tasks and datasets. Additionally, LoftQ is a model compression technique that focuses on reducing the size of pre-trained models while maintaining performance, utilizing different quantization methods like Uniform and NF2/NF4 to impact compression ratio, trainable parameters, and execution time of the models.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Compare and contrast the LoRA papers (LongLoRA, LoftQ). \"\n",
    "    \"Analyze the approach in each paper first. \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d02a4e2",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
